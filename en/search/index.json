[{"content":"As the heartbeat of Indonesia, Jakarta pulses with energy, culture and commerce. Exploring this bustling metropolis requires an efficient and reliable public transportation system. In this exploration, we delve into the intricacies of Jakarta\u0026rsquo;s public transportation, with a focus on TransJakarta buses in 2021, the latest public data from DKI Jakarta Transportation Agency on January 2024!\nUnderstanding TransJakarta\u0026rsquo;s Impact TransJakarta, Jakarta\u0026rsquo;s Bus Rapid Transit (BRT) system, stands as an important pillar of Jakarta\u0026rsquo;s transportation infrastructure. In 2021, TransJakarta plays a vital role in transporting millions of commuters across the city, acting as a lifeline for many daily activities.\nInsights from TransJakarta Data To comprehend the dynamics of TransJakarta\u0026rsquo;s operations, I leveraged data from 2021 to create a comprehensive tableau dashboard. This dashboard serves as a visual narrative that provides insights into key performance indicators, departure and destination routes, service frequency, and rough revenue estimation calculations.\nExplore the Interactive Dashboard Dive into the data and explore the trends shaping Jakarta\u0026rsquo;s public transportation landscape. You can also visit my dashboard here.\nKey Metrics: Travel Patterns: Visualize the ebb and flow of passengers throughout the month to provide valuable insights into busy months and potential areas for service improvement.\nRoute Efficiency: Understanding the efficiency of various routes can give an idea of overall system optimization and help identify potential congestion and passenger queue buildup.\nPopular Destinations: Mapping the most frequented stops and destinations will help in tailoring public transportation to the needs of the community.\nAddressing Challenges and Enhancing Services While TransJakarta has been a laudable initiative, the data analysis revealed several challenges that can be addressed for a more convenient travel experience:\nCongestion Hotspots: The need to identify areas and travel routes that experience frequent traffic congestion allows for targeted strategies to mitigate congestion-related delay issues and improve overall system efficiency.\nService Optimization: Evaluating the performance of different routes can guide decisions in reallocating resources to optimize service frequency and coverage.\nTechnology Integration: Utilizing technology for real-time service updates and information can improve the overall passenger experience.\nAdvocating for a Smarter Future Armed with insights from this data-driven analysis, we can advocate for smarter public transportation policies and investments. Technology integration, data-driven decision-making, and proactive planning can take Jakarta\u0026rsquo;s public transportation system to the next level.\nConnect with Me As we embark on this journey of data exploration, I invite you to connect with me. Share your thoughts, insights, and ideas on how we can collectively contribute to the evolution of Jakarta\u0026rsquo;s public transportation system. Let\u0026rsquo;s create a city where commuting is not just a necessity but a seamless and enjoyable experience.\nTogether, we can shape the future of Jakarta\u0026rsquo;s public transportation.\nPhoto by Ales Nesetril on Unsplash\n","date":"2024-01-18T00:00:00Z","image":"https://putraadp.github.io/Portfolio_Data/en/p/exploring-jakartas-public-transportation-landscape-a-data-driven-analysis/cover_hu3d03a01dcc18bc5be0e67db3d8d209a6_2212660_120x120_fill_q75_box_smart1.jpg","permalink":"https://putraadp.github.io/Portfolio_Data/en/p/exploring-jakartas-public-transportation-landscape-a-data-driven-analysis/","title":"Exploring Jakarta's Public Transportation Landscape: A Data-Driven Analysis"},{"content":"Welcome My New Website Portfolio.\nThis is the latest web page I\u0026rsquo;ve created since my previous website underscore48.github.io (no longer accessible) was published on a github account using my undergraduate college email account. Because I have now graduated for approximately 1.5 years, it is time for the email account to be deleted by the campus so that my previous github account can no longer be accessed.\nThis is my latest github account, the entire repository from my previous account has also been transferred to this account, so it can still be accessed on this latest account. As well as the portfolio pages that I have will be accessible again on this website. Although during 2023 there were not many updates. However, hopefully with the presence of a website with a more attractive design this will create new, more interesting pages, which of course still discuss various matters related to data science and feature updates from this website.\nRunning on :\nHugo.io Framework with Stack Theme by Jimmy Cai\nFeature :\nSearch Dark mode Quick Filter That\u0026rsquo;s all, thank you for coming.\nPhoto by Ales Nesetril on Unsplash\n","date":"2023-12-26T00:00:00Z","image":"https://putraadp.github.io/Portfolio_Data/en/p/new-portfolio/cover_hu3d03a01dcc18bc5be0e67db3d8d209a6_629725_120x120_fill_q75_box_smart1.jpg","permalink":"https://putraadp.github.io/Portfolio_Data/en/p/new-portfolio/","title":"New Website Portfolio Announcement"},{"content":"Overview Evaluate the risk and return of 10 stocks of The World\u0026rsquo;s Biggest Technology Companies 5 years ago to date based on annual Sharpe Ratios. Create a tool that functions to provide stock price predictions from companies that have the highest annual sharpe ratio evaluation results in previous evaluations. Machine learning with LSTM (Long Short Term Memory) model is used to predict stock prices with 99.3% accuration. All datasets are provided from Yahoo! Finance\u0026rsquo;s API. So, each daily runtime will provide the latest evaluation and prediction results. All graph in interactive for better visualization dan analyst experience. Code and Resources Used Python Version: 3.10.3\nPackages: numpy, pandas, tensorflow, matplotlib, yfinance, plotly, cufflinks, sklearn, keras, datetime Dataset source: Yahoo! Finance\u0026rsquo;s API\nNotebook Interactive View Here the link for Notebook Interactive Viewer\nCollecting Data First thing to do is to determine the 10 largest technology companies in the world, here I get a list from the Companiesmarketcap website (accessed on 5 september 2022), The world\u0026rsquo;s top 10 technology companies are:\nApple (USA) Microsoft (USA) Google (USA) Amazon (USA) Tesla (USA) Meta Platforms (USA) Taiwan Semiconductor Manufacturing Company (Taiwan) Tencent (China) Nvidia (USA) Alibaba (China) For comparison, the S\u0026amp;P 500 is used as a benchmark here. Then, the stock price dataset is taken from Yahoo! Finance\u0026rsquo;s API via the yfinance package, here we will use stock prices from the past 5 years.\nVisualized into a graph\nSharpe Ratio Annual Sharpe Ratio\nHere, we can see Microsoft, Apple, and Tesla have the highest annual sharpe ratio from the past 5 years data stock price.\nSo, if you want to buy some chips in the world\u0026rsquo;s technology Companies. The 3 Companies are the best choices based on the theory put forward by Professor William F. Sharpe.\nBuild Stock Price Machine Learning Model Because Microsoft has the highest sharpe ratio, it will be interesting if we can predict the stock price of Microsoft in the future. So, here I will create a machine learning model to predict stock prices from Microsoft. Here stock price of Microsoft from 5 years later. For interactive graph can be seen on this link\nIn this Model, I use machine learning regression model from Keras and TensorFlow for create Neural Network layers of Sequential with 2 LSTM (Long Short Term Memory) Layer and 2 Dense Layer. Here the summary of model :\nLayer (type) Output Shape Param # lstm_10 (LSTM) (None, 60, 128) 66560 lstm_11 (LSTM) (None, 64) 49408 dense_10 (Dense) (None, 25) 1625 dense_11 (Dense) (None, 1) 26 For each 1 prediction, using 60 stock prices from 60 active market days, or basically 2 months data stock price is for 1 predictions stock price. To get high prediction accuracy, model machine learning is trained using 1 batch data with 10 epoch. For 5 years dataset, it takes 5 minutes to complete tha training with loss around 0.0004, which is very nice.\nResult Prediction Price on 5 years Stock price Zoom for better visualization:\nHere we can see the predictions and the actual stock price is very close. Here the table result:\nAccuration MAE MSE 99,3 % 0.858 1.322 ","date":"2022-09-05T10:58:08Z","image":"https://putraadp.github.io/Portfolio_Data/en/p/risk-and-return-on-investment-in-the-10-biggest-tech-companies-in-the-world-predictions-for-the-future/invest_huebbfad1edfa8e198fe68a27ac6a615ca_276543_120x120_fill_q75_box_smart1.jpg","permalink":"https://putraadp.github.io/Portfolio_Data/en/p/risk-and-return-on-investment-in-the-10-biggest-tech-companies-in-the-world-predictions-for-the-future/","title":"Risk and Return on Investment in The 10 Biggest Tech Companies in The World \u0026 Predictions for The Future"},{"content":"Overview Create a tool that functions to provide rental price predictions for various properties based on the features possessed by the property. Machine learning regression model is used to predict property rental rates. Using data sets of San Francisco,CA property rental price. Code and Resources Used Python Version: 3.10.3\nPackages: pandas, numpy, sklearn, math Dataset source: Datacamp case study\nProcessing Data After collect Dataset, I need to check and clean up the dataset to make sure no missing and anomaly values on Dataset before creating a Machine Learning model. I made the following changes and created the following variables:\nImport Dataset to Data Frame with pandas. Fix datatype of every feature. Check and dealing with missing values. Cleaning object columns and labeled. Analyze and resolve anomaly values. Data Comparation Before resolve anomaly After resolve anomaly Property Location Data Engineering From property location we can calculate the distance of property location to downtown. So, i add new feature \u0026lsquo;distance\u0026rsquo;. Here the heat map of linear correlations\nModel and Result Rank Model Score MAE RMSE 1 RandomForestRegressor 0.575 72.24 132.62 2 GradientBoostingRegressor 0.570 73.16 133.41 3 LinearRegression 0.445 84.19 151.52 4 Ridge 0.445 84.19 151.51 5 Lasso 0.445 83.69 151.59 6 DecisionTreeRegressor 0.225 91.96 179.02 From several models, RandomForestRegressor obtained the highest accuracy score, with score 57.5%. and with optimization get results:\nModel Score MAE RMSE RandomForestRegressor 0.622 69.23 125.13 EDA Feature importance\nHere we can see that the high and low price of property rental is strongly influenced by the number of rooms and sligly influenced by property location.\nMore rooms, more prices.\nDistance? doesn\u0026rsquo;t really matter\nOutput Sample Here, sample distribution of predict and actual price rent.\n","date":"2022-06-19T10:58:08Z","image":"https://putraadp.github.io/Portfolio_Data/en/p/property_rental_price_prediction/rentals_huebbfad1edfa8e198fe68a27ac6a615ca_300799_120x120_fill_q75_box_smart1.jpg","permalink":"https://putraadp.github.io/Portfolio_Data/en/p/property_rental_price_prediction/","title":"Property Rental Price Prediction"},{"content":" Create a tool that has a function to evaluate and classify every credit card transaction submitted whether it is valid or fraudulent. A tool made using Machine Learning with a 100% accuracy rate to avoid fraud and laundering that can harm Customers and Bank. Using data sets published by UCI Machine Learning Repository (Credit Approval Data Set). The Machine Learning model used is Logistic Regression with parameter optimization, maximum iterations and tolerance values, using Grid Search Cross Validation to produce model with the best performance. Code and Resources Used Python Version: 3.10.3\nPackages: pandas, numpy, sklearn\nDataset source: https://archive.ics.uci.edu/ml/datasets/credit+approval\nData Cleaning After collect Dataset from UCI Machine Learning Repository, I need to check and clean up the dataset to makesure no missing values on Dataset before creating a Machine Learning model. I made the following changes and created the following variables:\nImport Dataset to Data Frame with pandas. Change the name columns of Dataset to make it easier to understand. Check for missing values owned by the Dataset. Delete columns that are judged not to be related to the target. Clean up string missing values with most common values of each columns. Preprocessing Data After Dataset is clean, the dataset is not quite ready to fit into the model because it is still at a different scale in each column. So, I made the following changes and created the following variables:\nSplitting dataset for test (30%) and training (70%) purpose. Create the dummies dataset. Scaling dummies dataset with values betwen minimum 0 to maximum 1. The dataset is ready to fit into the model. Model Building Because approval credit card must be fast and accurate, the machine learning classification model must be quick and strong enough. So, Logistic regression with parameter optimization can be suitable for use.\nCreat multiple case of parameters for Logistic regression. Fitting dummies dataset training to model via Grid Search Cross Validation. Estimator score evaluation and save the model with best estimator. Predict dummies dataset test and show the result on Confusion Matrix Model performance Best parameter:\nMaximum Iteration = 100 Tolerance value = 0.01 Best Score:\nEstimator score = 1.0000 (Prediction 100% accurate) ","date":"2022-06-13T10:58:00Z","image":"https://putraadp.github.io/Portfolio_Data/en/p/credit-card-transaction-validation/cover_hub9d0f3a6e618ae9499dcc10dd273d8ec_193213_120x120_fill_box_smart1_3.png","permalink":"https://putraadp.github.io/Portfolio_Data/en/p/credit-card-transaction-validation/","title":"Credit Card Transaction Validation"}]